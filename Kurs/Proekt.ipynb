{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый этап - функционал болталки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузка библиотек\n",
    "import os\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
    "import string\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from string import punctuation\n",
    "import annoy\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Болталка будет смесью встроенного функционала dialogflow и собственноручного поиска вопроса и ответа из prepared_answers.txt и Otvety.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c2764a3d0e42aab6315c03fc691bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Предобработка файла с ответами \n",
    "\n",
    "question = None\n",
    "written = False\n",
    "\n",
    "with open(\"prepared_answers.txt\", \"w\") as fout:\n",
    "    with open(\"Otvety.txt\", \"r\", errors=\"ignore\") as fin:\n",
    "        for line in tqdm_notebook(fin):\n",
    "            \n",
    "            if line.startswith(\"---\"):\n",
    "                written = False\n",
    "                continue\n",
    "            if not written and question is not None:\n",
    "                fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
    "                written = True\n",
    "                question = None\n",
    "                continue\n",
    "            if not written:\n",
    "                question = line.strip()\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предобработка текста\n",
    "def preprocess_txt(line):\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf9157edad24fb0bc45eaac4962bf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Предобработка для модели\n",
    "\n",
    "sentences = []\n",
    "\n",
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)\n",
    "c = 0\n",
    "\n",
    "with open(\"Otvety.txt\", \"r\", errors=\"ignore\") as fin:\n",
    "    for line in tqdm_notebook(fin):\n",
    "        spls = preprocess_txt(line)\n",
    "        sentences.append(spls)\n",
    "        c += 1\n",
    "        if c > 500000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим модель на наших вопросах\n",
    "sentences = [i for i in sentences if len(i) > 2]\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, min_count=1, window=5)\n",
    "model.save(\"w2v_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3971a7f619af4594b9a4f2c626dacc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Через библиотеку annoy проходим по всем ответам и вычисляем усредненый вектор, результаты сохраняем\n",
    "index = annoy.AnnoyIndex(100 ,'angular')\n",
    "\n",
    "index_map = {}\n",
    "counter = 0\n",
    "\n",
    "with open(\"prepared_answers.txt\", \"r\") as f:\n",
    "    for line in tqdm_notebook(f):\n",
    "        n_w2v = 0\n",
    "        spls = line.split(\"\\t\")\n",
    "        index_map[counter] = spls[1]\n",
    "        question = preprocess_txt(spls[0])\n",
    "        vector = np.zeros(100)\n",
    "        for word in question:\n",
    "            if word in model.wv:\n",
    "                vector += model.wv[word]\n",
    "                n_w2v += 1\n",
    "        if n_w2v > 0:\n",
    "            vector = vector / n_w2v\n",
    "        index.add_item(counter, vector)\n",
    "            \n",
    "        counter += 1\n",
    "\n",
    "index.build(10)\n",
    "index.save('speaker.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраним список ответов\n",
    "pickle.dump(index_map, open(\"index_map\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Загрузим резульаты модели\n",
    "ft_index = annoy.AnnoyIndex(100, 'angular')\n",
    "ft_index.load('speaker.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй этап - продуктовая часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = pickle.load(open(\"index_map\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(punctuation)\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "morpher = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузим датасет с товаркой\n",
    "shop_data = pd.read_csv(\"ProductsDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Релизуем предобработку текста для классификатора\n",
    "import re\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"nan\", \"\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in exclude]\n",
    "    return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединим в один стобец название с описанием продукта и предобработаем\n",
    "shop_data['text']= shop_data['title'].apply(preprocess_text) + ' ' + shop_data['descrirption'].apply(preprocess_text)\n",
    "# Целевая переменная для классификации\n",
    "shop_data['target'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дополним датасет данными\n",
    "data = pd.DataFrame(list(index_map.items()),\n",
    "                   columns=['index', 'text'])\n",
    "data = data[data.index < len(shop_data)]\n",
    "data['target'] = False\n",
    "data = data.drop(['index'], axis=1)\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "data_class = pd.concat([shop_data, data], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model, metrics, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делим выборку\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(data_class['text'], data_class['target'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизируем текст при помощи TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer().fit(train_x.values)\n",
    "xtrain_tfidf = vectorizer.fit_transform(train_x)\n",
    "xvalid_tfidf = vectorizer.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:  0.9935298750984585\n"
     ]
    }
   ],
   "source": [
    "# Обучим классификатор \n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_tfidf, train_y)\n",
    "predictions = classifier.predict(xvalid_tfidf)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем модель классификатор\n",
    "pickle.dump(classifier, open(\"text_classifier\",'wb'))\n",
    "# Загружаем модель классификатор\n",
    "classifier_model = pickle.load(open(\"text_classifier\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6675d090c34d11a1d441d4568047b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=35548.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Создадим список слов из названия и описания продуктов\n",
    "titles = []\n",
    "\n",
    "for line in tqdm_notebook(data.text):\n",
    "    spls = preprocess_txt(line)\n",
    "    titles.append(spls)\n",
    "\n",
    "# Векторизируем список слов при помощи модели Word2Vec и сохраним модель \n",
    "titles = [i for i in titles if len(i) > 2]\n",
    "product_model = Word2Vec(sentences=titles, vector_size=100, min_count=1, window=5)\n",
    "product_model.save(\"w2v_product_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сложим в индексы используем библиотеку annoy. \n",
    "product_index = annoy.AnnoyIndex(100 ,'angular')\n",
    "\n",
    "index_map_titles = {}\n",
    "counter = 0\n",
    "\n",
    "for quest in shop_data.title:\n",
    "    n_w2v = 0\n",
    "    index_map_titles[counter] = str(shop_data.product_id[counter]) + ' ' + shop_data.title[counter]\n",
    "    question = preprocess_txt(quest)\n",
    "    vector = np.zeros(100)\n",
    "    for word in question:\n",
    "        if word in product_model.wv:\n",
    "            vector += product_model.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    product_index.add_item(counter, vector)\n",
    "        \n",
    "    counter += 1\n",
    "\n",
    "product_index.build(10)\n",
    "product_index.save('shopx.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для поиска продуктов\n",
    "def find_product(question):\n",
    "    preprocessed_question = preprocess_txt(question)\n",
    "    n_w2v = 0\n",
    "    vector = np.zeros(100)\n",
    "    for word in preprocessed_question:\n",
    "        if word in product_model.wv:\n",
    "            vector += product_model.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    query_index = product_index.get_nns_by_vector(vector, 1)\n",
    "    return index_map_titles[query_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для болталки возьмем обученную ранее модель и созданные индексы\n",
    "answer_model = Word2Vec.load(\"w2v_model\")\n",
    "answer_index = annoy.AnnoyIndex(100 ,'angular')\n",
    "answer_index.load('speaker.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для ответов на запросы \"болталки\"\n",
    "def find_answer(question):\n",
    "    preprocessed_question = preprocess_txt(question)\n",
    "    n_w2v = 0\n",
    "    vector = np.zeros(100)\n",
    "    for word in preprocessed_question:\n",
    "        if word in answer_model.wv:\n",
    "            vector += answer_model.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    query_index = answer_index.get_nns_by_vector(vector, 1)\n",
    "    return index_map[query_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для ответа на запрос пользователя\n",
    "def get_answer(answer):\n",
    "    ans = pd.Series([answer], dtype=\"string\")\n",
    "    answer_tfidf = tfidf_vec.transform(ans) # Векторизируем запрос при помощи TfidfVectorizer\n",
    "    if classifier.predict(answer_tfidf)==1: # Если запрос был продуктовым, возвращаем id и название продукта\n",
    "        return find_product(answer)\n",
    "    else:\n",
    "        return find_answer(answer) # иначе отправляем запрос в болталку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подключение бота\n",
    "updater = Updater(token='1058946299:AAFn0ESsBoqdzscvF-tYo0wcZvAUS_iJMCw') # Токен API к Telegram\n",
    "dispatcher = updater.dispatcher\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'bot-nlp-igor-pemi-ff9190fe2c7d.json'# скачнный JSON\n",
    "\n",
    "\n",
    "DIALOGFLOW_PROJECT_ID = 'bot-nlp-igor-pemi' #PROJECT ID из DialogFlow \n",
    "DIALOGFLOW_LANGUAGE_CODE = 'ru' # язык\n",
    "SESSION_ID = 'Bot_nlp_igor_bot'  # ID бота из телеграма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startCommand(bot, update):\n",
    "    bot.send_message(chat_id=update.message.chat_id, text='Добрый день')\n",
    "\n",
    "def textMessage(bot, update):\n",
    "    \n",
    "    input_txt = preprocess_txt(update.message.text)\n",
    "    vect = vectorizer.transform([\" \".join(input_txt)])\n",
    "    prediction = lr.predict(vect)\n",
    "    \n",
    "    if prediction[0] == 1:\n",
    "        vect_ft = embed_txt(input_txt, idfs, midf)\n",
    "        ft_index_shop_val = ft_index_shop.get_nns_by_vector(vect_ft, 5)\n",
    "        for item in ft_index_shop_val:\n",
    "            title, image = index_map_shop[item]\n",
    "            bot.send_message(chat_id=update.message.chat_id, text=\"title: {} image: {}\".format(title, image))\n",
    "        return\n",
    "    vect_ft = embed_txt(input_txt, {}, 1)\n",
    "    ft_index_val, distances = ft_index.get_nns_by_vector(vect_ft, 1, include_distances=True)\n",
    "    if distances[0] > 0.2:\n",
    "        print(distances[0])\n",
    "        bot.send_message(chat_id=update.message.chat_id, text=\"Повторите вопрос\")\n",
    "        return\n",
    "    bot.send_message(chat_id=update.message.chat_id, text=index_map[ft_index_val[0]])\n",
    "    \n",
    "start_command_handler = CommandHandler('start', startCommand)\n",
    "text_message_handler = MessageHandler(Filters.text, textMessage)\n",
    "# Добавляем хендлеры в диспетчер\n",
    "dispatcher.add_handler(start_command_handler)\n",
    "dispatcher.add_handler(text_message_handler)\n",
    "# Начинаем поиск обновлений\n",
    "updater.start_polling(clean=True)\n",
    "# Останавливаем бота, если были нажаты Ctrl + C\n",
    "updater.idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
